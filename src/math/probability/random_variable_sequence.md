# 独立随机变量序列

## Kolmogorov's Zero-One Law（柯尔莫哥洛夫 零一律）

设 \\((\Omega, \mathcal{F}, \mathbb{P})\\) 是一个概率空间（测度空间），定义在该概率空间上的无限独立同分布随机变量序列 \\(\\{ x_n\\}_{n=1}^{\infty}\\)
\\[
\\{X_n : \Omega \to \mathbb{R}, \quad n = 1,2,3,\dots\\}
\\]

以无限次投掷一枚公平硬币为案例说明符号对应的具体东西，正面用 1 表示，反面用 0 表示，首先是样本空间

\\[ 
   \Omega = \\{ w_n:w_n=(a_k: a_k \in \\{0, 1\\}, \quad k=1,2,3,\dots), \quad n=1,2,3,\dots\\}
\\]
也就是说样本空间中的每个元素都是一个
\\[
  w_n = (a_1, a_2, a_3, \dots)
\\]

可测集的集合系（事件空间，事件系统，σ-代数） \\( \mathcal{F} \\) 可以表示为

\\[
    \mathcal{F} = \sigma(\Omega)
\\]

这个生成过程，比较复杂，可以先理解为幂级集合系。

对于抛硬币，为正的概率为 \\(p=\frac{1}{2}\\) 事件空间的测度（概率测度）为

\\[
    P(A \in \sigma(\Omega) ) = \sum_{w \in A} \Pi_{i=1}^{n} p^{a_i}p^{1-a_i} 
    = \sum_{w \in A} \frac{1}{2^n}
\\]

可以看出，如果 n 趋近于无穷大，单点事件 \\(\\{ w_n \\} \\)的概率为 0 。

我们再在 测度空间 \\((\Omega, \mathcal{F}, \mathbb{P})\\) 的基础上定义随机变量序列

\\[
    (X_k: \Omega \to \\{0, 1\\}; \quad X_k(w)= w[k] = a_k)
\\]
也就是第 k 次抛硬币出现的情况（这样定义，我们才能获得一个无限同分布的随机变量序列）。

定义 **尾 σ-代数**（tail σ-algebra）为：
\\[
\mathcal{T} = \bigcap_{k=1}^\infty \sigma(X_k, X_{K+1}, \dots) = \bigcap_{k=1}^\infty \sigma(X_m: m \geq k)
\\]

注意是并集，换句话说，\\(\mathcal{T}\\) 中的事件只与无限远的未来有关，而与任意有限个 \\(X_1, X_2, \dots, X_k\\) 无关。

如何理解 尾 σ-代数？

最关键的是理解生成过程： 

\\[
\sigma(X_k, X_{K+1}, \dots) = 
\sigma({ X_k^{-1}(B_k) \bigcap X_{k+1}^{-1}(B_{k+1}) \dots; B_k \in {0, 1}})
\\]

因此
\\[
    w \in \sigma(X_k, X_{K+1}, \dots) \to w \in \Omega
\\]

也就是说尾 σ-代数 的样本空间仍然是 \\(\Omega\\)，但是尾 σ-代数中的事件之间的差别只能是远处的无穷项决定。

以无限次投掷一枚公平硬币为案例，我们描述一个事件（注意事件的本质是集合），正反次数出现的频率极限是为 0.5。

\\[
   A = \\{w: w \in \Omega, \lim_{n \to \infty} \frac{1}{n} \sum_{k=1}^{n} X_k = \frac{1}{2}\\} \in \mathcal{T}
\\]

正反次数出现的频率 是和 远处的无限个样本相关的，于过去的样本没有关系。

**柯尔莫哥洛夫零一律描述为**：

如果 \\(\\{X_n\\}\\) 是独立随机变量序列，那么对于所有属于尾 σ-代数 \\(\mathcal{T}\\) 的事件 \\(A\\)，有：
\\[ p(A) \in \\{0, 1\\} \\]

由于尾事件与任何有限个 \\(X_n\\) 无关，因此其概率在给定任何有限信息后都不改变。因此尾事件对所有有限信息都是 **条件独立的**。

\\[ P(A) = P(A|\sigma(X_1,..., X_k) ) \\]

根据条件期望的定义（以及概率分布），可以得到：
\\[ P(A) = E[1_A|\sigma(X_1,..., X_k)] \\]

其中 \\( 1_A \\)  就是简单函数构建的可测函数（随机变量）

对 k 求极限（马氏收敛定理）
\\[ P(A) = \lim_{k \to \infty } E[1_A|\sigma(X_1,..., X_k)] = 1_A\\]

也就是说\\( P(A) \in \\{0, 1\\} \\) ，这说明要么 𝐴 几乎必然发生（概率 1），要么几乎必然不发生（概率 0）。

随着你知道的硬币次数越来越多，你的预测会逐渐变得确定，我们观测到正反面几乎各种一半。也就是说，无论我们什么时候抛硬币足够多，都可以得到接近0.5的概率（正反面各占一半的概率是为 1 ）

零一律告诉我们，在无限独立实验下，某些问题根本没有不确定性（比如，均值，方差），要么必然发生，要么必然不发生。


## 大数定律

随着试验次数的增加，样本均值会趋近于总体的数学期望。

样本均值 \\(\overline{X}_n\\) 以概率收敛到总体期望 \\(\mu\\)，即**弱大数定律成立**。

\\[
\lim_{n \to \infty} P(|\overline{X}_n - \mu| \geq \varepsilon) = 0.
\\]

**样本均值几乎处处收敛到期望 \\(\mu\\)**，即**强大数定律**
\\[
P\left( \lim_{n \to \infty} \overline{X}_n = \mu \right) = 1,
\\]

- 大数定律为频率解释概率提供数学基础。保证了大量重复试验中样本均值会接近理论期望，从而使概率的频率意义成立（指示函数，发生和不发生）。
- 在实际数据分析和统计推断中，利用大数定律可以相信样本均值是总体均值的合理估计，支撑样本均值的可靠性。
- 通过大量随机样本计算平均值，用来估计复杂问题的期望值或概率，是数值计算的重要工具（数值模拟和蒙特卡洛方法）。
