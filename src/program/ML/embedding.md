## embedding

字符的原始编码（比如，UTF-8）的设计目标是将不同的字符区分开（当然会考虑兼容性和存储效率问题），并不适合用于字符的意义判断，比如是否相似（中文字符），如何组成一个有意义的词（比如，英文单词）。

因此，我们需要将字符串编码成便于后续计算的数据（一般用向量来做），主要任务有以下几个。
- 将字符串分成一个个小片段（最小的，有意义的片段）。
- 将这些片段表示为一个向量。

这个过程要做 `embedding`，`embedding`之后的数据叫做 `token`，英文单词比较多，一般不会为每个单词都生成`token`，而是进行分词切片（子词），一个单词因为不同的形态会产生不同的词，如由`look`衍生出的`look, looked`，显然这些词具有相近的意思，但是在词表中这些词会被当作不同的词处理，一方面增加了冗余，另一方面也造成了大词汇量问题。

分词字词之后，单词中止符也会被作为一个单独的子词。这样解码的时候，就很容易进行，如果相邻子词间没有中止符，则将两子词直接拼接，否则两子词之间添加分隔符。

再一次大模型调用钟，**首先**我们要把用户的输入（问题） `string` 进行切片，切片之后叫做 token，有可能是单词，也有可能只是单词的片段（to, world, meta...），对于视频和图片而言，tokens可能代表一小块图片或者一小段语音，而每个token对应一个向量，也就是一组数字。
\\[
to = \begin{bmatrix}
1.2 \\
0.1 \\
0.3 \\
... \\
2.3
\end{bmatrix}
\\]

我们把这个过程叫做 `Embedding`。
\\[ vector = Embedding(token) \\]

如何构建`Embedding`，我们的目标是要让该向量，代表字符的意义（向量就是意义），这种向量可以想象为高维空间的点，字符(token)之间越相似，那么向量在空间中就会越接近。

\\[E(雌) \approx E(雄) + E(女) - E(男)\\]

那么如何编码token的？又是如何确定维度的（GPT-3有12288个维度）？

模型有预设的词汇库（~50k），**Embedding Matrix**，就是每个词都对应一个向量。
