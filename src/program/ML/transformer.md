# Transformer

Transformer 的核心思想是通过**自注意力机制（Self-Attention）**和**多层堆叠的神经网络**进行语言建模。
 
先不管这些高大上的算法词汇，先用技术上宏观梳理数据流，
那么，基于 Transformer 构建文本生成的核心数据流是什么？

输入是一个字符串，比如'北京位于'，输出这是这个字符串后可能的字的概率，比如 '北'：20%，'京': 10%。

如何做到求概率，第一步当然是量化这些字符。

有了数字，才能做计算。

一个输入字符串，\\( [北, 京, 位, 于] \\) 会被描述为一个二维的数组（上下文长度，GPT: 2048）。
**接着**输入到**注意力机制模块**模型中，该模型允许向量（token）之间进行相互约束，来重新更新每个向量，
为什么要这么做，因为一个词在不同的语境下应该有不同的含义（**含义就是向量*）。比如 `问问题` 中的两个 `问`显然不是一个意思，
因此需要根据上下文，更新向量，也就是说，上下文的向量会拉扯当前的向量，使得它指向新的方向。

**然后**将更新后的每个向量，输入到**多层感知机**中，向量之间没关系，这是在提取每个字符本身的含义（这是中文吗，这是一个动词吗？这是数字吗？输出的维度暗含了我们考虑的方面有多少）。

**然后重复之前的过程**

Attention -> Preception -> Attention -> Preception -> ... ->

最后的目的，是将整个字符串的所有关键含义在最后一个向量中表达出来。

然后再对最后一个向量作为输入，与**Unembedding Matrix**相乘（类似每一个词与最后一个向量的相识度），
再进行归一化处理，得到所有token的概率分布。

\\[ W_U \times x_{out} = y_{out}\\]

## 自注意力机制

那么什么是自注意力机制？

## 多层堆叠的神经网络
