# 机器学习

机器学习是一种寻找数据中蕴含的**关系**（ \\(y = f(x)\\) ）的方法。与物理思维不同，机器学习是依靠数据去拟合参数，从而抽取数据中蕴含的可能**关系**。

物理思维是通过基本定律（因果）的约束，去推导**关系**。比如已知加速度和速度的定义
\\[ a = \frac{dv}{dt} \\]
\\[ v = \frac{ds}{dt}\\]
可以得到位移的公式（位移与时间的**关系**）
\\[ s = \frac{1}{2}at^2 + v_0 t + s_0 \\]

**数据驱动**的含义是指，不去在意因果，只关注**关系**。

> 因果是人类思维执着的东西，虽然机器学习在一些领域能够拟合出较好关系，我们仍然会执着于解释，为什么机器学习有这样的能力。大量的神经节点的堆叠真的能表现出一定的智慧（什么是智慧？表现出的智慧具体是指什么？），还是只是某种更为高效的信息存储手段（记忆）和更为强大的信息检索能力（所谓的文本生成）。

**关系**的准确描述需要数学语言，不管你是否想给这个关系赋予其物理意义。经典的统计回归算法会给定一个函数形式，如何给定？先感性地观察样本（肉眼看），可以大致看出变量之间满足什么关系，比如线性型、指数型、对数型、多项式型、周期型。然后给出一个可能的函数形式（可以是复合函数），该函数会有一些待定的参数，最简单函数形式就是多元线性回归

\\[ y = a_1 x_1 + a_2 x_2 + ... + a_n x_n + b\\]

再通过一组的样本，去确定参数 \\(a\\) 和 \\(b\\)。这样基本上能够抓住主要矛盾（毕竟肉眼就可以看出来），但是很难提升模型的精度（给定未知样本，得到的 `y` 与实际观测的相差可能很大）。

很多物理定律的获得其实也采用了这种方法，不过其待确定的参数通常很少，同时物理的视角更聚焦公式的意义，也就是为什么公式会是这样的（机器学习关注的是新样本来了准不准，只要准就行，并不关注公式及其参数的意义，只要在不准的时候，才会去审视为什么）。比如万有引力定律描述两个质量体之间因引力而相互吸引的关系，这个定律并非从更基本的定律严格推导而来，而是通过观察、实验（如开普勒行星运动定律）和**逻辑推理总结**出来的经验定律，也存在一个待定参数（万有引力常数 \\(G\\) ，卡文迪许扭秤实验获得大量样本，也会用最小二乘法去确定参数值）。万有引力定律能够预测很多天文现象，但是我们并不满足于此，而是更执着于为什么会是这样（广义相对论）。

同时，不是每次都能轻易的看出来关系的大致函数形式（系统物理机制不完全清楚或过于复杂、数据庞杂，但难以建立明确数学方程、存在高度非线性或高维特征空间），另一类机器学习算法，并不预设 \\(f\\) 的形式，而是借助如神经网络、支持向量机、随机森林等算法，去可以去拟合任意未知函数形式（也是目前机器学习这么火的原因），类似傅里叶级数，通过基本函数的组合（三角函数），从而可以拟合任意函数（函数的性质有限制，但是应该不多）。

对于数据集中蕴含的复杂关系的情况，我们可以使用这种方案。比如对于语言、文字、图片，我们每个人去看，去听，去观察之后，都能得到差不多的结论（至少在语义或者形式上不会有太大歧义），说明语言识别，图片识别一定存在某种关系，但是这种关系从直觉上很难给出一个函数形式，那么就可以用神经网络去做拟合。

为了能更好地解决更复杂的问题，出现了越来越复杂的拓扑结构，同时，也使得参数的拟合变得越来越困难，并且消耗巨量资源，人力和资源的投入是否会越来越没有性价比，我也十分好奇这是否是一条有深度的、可持续发展的路。因此决定研究一下相关技术，希望自己不会迷失在这铺天盖地的宣传中。

在我理解，作为高级智慧生物，我们真正智慧的地方是抽象的能力，从有限的经验中，总结出更一般的模型（归纳），这个抽象不是一般意义上的堆积性的归纳，而是会创造脱离经验的，蕴含意义更加广泛的符号，用这些符号来描述关系，期待得到更具普适性的认知模型，虽然也存在黑天鹅的问题，模型并不一定就是宇宙的真理，就像牛顿力学被相对论替换一样，我们抽象出了的模型，只能解释我们目前收集到的所有经验，并可能能预测一些未知的经验，但是这种抽象之后的模型，是人类目前最能拿得出手的东西。模型成体系的出现，并在之后的实践过程中，被大量未知的经验反复的认证过，就会形成一个完整的理论，一门学科，比如物理、化学、生物。

那么机器的智慧，到底是什么样的呢？
