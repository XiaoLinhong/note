# 大气状态预测

要对大气状态进行预报，我们需要构建模拟器:

\\[ \Phi: (X^{t-\Delta t}, X^{t}) \to  \hat{X}^{t+\Delta t}; \quad 
X^{t} \in  R^{(V_s + V_a \times C) \times H \times W} \\]

其中 \\(X\\) 表示大气状态变量（温度、气压等），\\(t\\) 表示当前事件，\\(\Delta t\\) 表示离散时间间隔， \\(V_s\\) 表示地表变量数（包括静态变量：地形，海陆，土壤），\\(V_a\\) 表示气压场变量数， \\(C\\) 表示气压层， \\(H\\) 表示维向网格数， \\(W\\) 表示经向网格数。

为了预报更长时间，用预报的数据作为输入，进行迭代计算

\\[ \hat{X}^{t+k\times \Delta t} = \Phi(\hat{X}^{t+(k-2) \times \Delta t}, \hat{X}^{t+(k-1) \times \Delta t} )  \\]

## 模式架构

相比于文本数据，在多样化的气象数据集上训练大型模型是一项更具挑战性的任务，因为气象数据的异质性更强。语言数据通常是同质的，而气象数据集则包含不同的变量、气压层次以及分辨率。此外，由于存储完整模拟输出的成本较高，许多数据来源仅通过在上述一个或多个维度上减少数据集大小的方式，提供部分数据子集。为了适应这种异质性的数据集，我们设计了一个灵活的编码器，将具有不同结构的数据集映射为标准化的三维张量，以便输入到模型的主干网络中。


## ViTs 
ViTs 是 Vision Transformers 的缩写，它是将 Transformer 架构 应用于 计算机视觉任务（如图像分类、目标检测、语义分割等）的一类模型。

将图片切割为 patch ，一张图片可能有10个 patch， 然后把每个 patch 类比成一个token，这样就能用 Transformer 去 抓取 patch 之间的空间关系。

patch 分割多大？ 越多的patch，计算时间越长（全局 自注意力）。

## Swin Transformer 

Swin Transformer（Shifted Window Transformer）是 Vision Transformer 的一种改进版本。

相较于原始 ViT（对整幅图像进行全局注意力），Swin Transformer 引入了 局部窗口注意力 和 滑动窗口机制，以更好地兼顾计算效率和建模能力。

## U-Net

通过卷积，把空间相关的信息，抽取到一个向量中（隐变量维度增加），通过池化，降低空间维度。迭代上述过程，也就是 U-Net 的 encode部分。在 decode 部分通过上采样，扩张空间维度，然后和 decode 中 卷积的输出部分合并到一起，在通过 decode中的卷积，将 隐变量 的信息还原到空间层面，迭代上述过程。

