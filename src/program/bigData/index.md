# 大数据

当数据量达到海量（TB级别以上），并且还在持续增长，同时数据类型也变得多样化时，我们称之为大数据。
传统的数据库技术已经很难满足这些海量数据在存储和计算上的高效处理需求。

解决海量数据带来的挑战的技术，被称为大数据技术。

一个很自然的思路就是采用分布式架构，也就是将数据的计算和存储能力分散到多台机器上进行协同处理（这与传统的磁盘阵列方式不同）。目前，主流的大数据技术实现大多来自 Apache 软件基金会。

Apache Hadoop 是大数据技术的一种实现方式。Hadoop 是一个框架，允许使用简单的编程模型跨计算机集群对大型数据集进行分布式处理。最小 Hadoop 集群包含三个组件，分别为 作为分布式存储的 Hadoop 分布式文件系统 (HDFS)、用于调度和管理集群的 Hadoop YARN 以及作为执行引擎进行并行处理的 Hadoop MapReduce。

另一方面，Apache Spark 是另一种可选的数据处理执行引擎（**更快**）。关键区别在于，Spark 使用弹性分布式数据集 (RDD) 并将其存储在内存中，从而显著提升了性能，远超 MapReduce。Spark 可以独立运行并实现基本功能。然而，与 Hadoop 结合使用时，Spark 可以利用 Hadoop 的功能进行集群管理、容错、安全性等。

Hive 是一款分析工具，它利用存储在 HDFS 中的数据，并使用 Spark 或 MapReduce 作为执行引擎。这使得用户可以使用熟悉的 SQL 语句来分析数据，从而帮助开发人员更轻松地从数据中挖掘洞察。

在大数据的架构中，需要到把数据导入到 HDFS 系统中（为了存储大量数据），然后用 MapReduce 或者 Spark 对文件进行处理（对大量数据进行计算），或者用 Hive 等应用工具进行数据吹，形成新的文件，然后再把分析、统计凝练之后的数据导出 HDFS 系统（处理之后的数据会小很多，在HDFS中直接操作太慢了）。 

在这个过程中，可能会涉及到 Flume 和 Sqoop 对非结构化数据和结构化数据进行导入导出。

因此整个流程涉及到 数据收集，数据处理，数据导出，这些任务之间有相互依赖关系，
可以用 Azkaban 进行任务流管理 （cron + 依赖管理）

## 搭建测试环境

了解系统内部（底层）的工作原理是非常重要的。
为了做到这一点，我们首先需要搭建一个成本低、灵活性高的开发环境。

在这种情况下，使用 Docker 来启动和管理大气数据生态系统中的各个组件，无疑是一个理想的选择。它不仅方便搭建环境、隔离依赖，还能帮助我们快速测试和理解各个模块的运行机制。

具体参考该[仓](https://github.com/XiaoLinhong/hadoop-docer)。
